from torch.utils import data
import torch
from preprocess import dataprocess
import time
from model import MLP256, MLP512
from torch import nn
starttime = time.time()

features, labels, user_scal, item_scal = dataprocess()
####################################################################################
def load_array(data_arrays, batch_size, is_train=True):  # @save
    """构造一个PyTorch数据迭代器。"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 20
epoch_num = 2000
data_iter = load_array((features, labels), batch_size)
####################################################################################
net = MLP256()
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)
net.apply(init_weights)

loss = nn.MSELoss()
optimizer = torch.optim.SGD(net.parameters(), lr=0.22)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epoch_num)
####################################################################################
num_epochs = epoch_num
for epoch in range(num_epochs):
    epoch_time = time.time()
    for X, y in data_iter:
        l = loss(net(X), y)
        optimizer.zero_grad()
        l.backward()
        optimizer.step()
    scheduler.step()
    l = loss(net(features), labels)
    if epoch % 10 == 9:
        print(f'epoch {epoch + 1}, loss {l:f}, 1 epoch time {time.time()-epoch_time:.2f}')
####################################################################################
# testx = np.array([[3000,4500,3,1]])
# testx = testx.astype(np.float32)
# testx = user_scal.transform(testx)
# testy = net(torch.tensor(testx))
# testy = testy.detach().numpy()
# testy = item_scal.inverse_transform(testy)
# print(testy)

torch.save(net.state_dict(), './data/mlp.params')
print('---> mlp.params (generated by train.py) <--- saving succeeded.')
print('3.  ------------->  train down : {:.2f} seconds'.format(time.time()-starttime))
##################################################################################

