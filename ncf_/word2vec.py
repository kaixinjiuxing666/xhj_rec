import jieba
from gensim import corpora
from gensim import models

def v2v(lst):
    ch_lst = lst
    cut_words = map(lambda s: list(jieba.cut(s)), ch_lst)
    word_list = list(cut_words)
    #####################################################################
    # 赋给语料库中每个词(不重复的词)一个整数id
    #dictionary = corpora.Dictionary(word_list)
    dictionary = corpora.Dictionary.load('./data/all_ch_dict.dict')
    print('---> all_ch_dict.dict (generated by word2vec_train.py) <--- loading succeeded.')
    #new_corpus = [dictionary.doc2bow(text) for text in word_list]
    # 元组中第一个元素是词语在词典中对应的id，第二个元素是词语在文档中出现的次数
    #print(new_corpus)
    # 语料库中每个词对应的id
    #print(dictionary.token2id)
    ###################################
    # tfidf = models.TfidfModel(new_corpus)
    # tfidf.save("ch_all_model.tfidf")
    # 载入模型
    tfidf = models.TfidfModel.load("./data/ch_all_model.tfidf")
    print('---> ch_all_model.tfidf (generated by word2vec_train.py) <--- loading succeeded.')

    name_lst = []
    tfidf_vec = []
    for i in range(len(ch_lst)):
        string = str()
        for st in word_list[i]:
            string+=st
            string+=' '
        string_bow = dictionary.doc2bow(string.split())
        string_tfidf = tfidf[string_bow]
        tfidf_vec.append(string_tfidf)
        h = 0
        for j in string_tfidf:
            h+=j[1]
            h = h/len(string_tfidf)
        name_lst.append(round(h,5))
    #print('2.  ------------->  word2vec down !')
    return name_lst

def ordinal():
    file = './data/train_data_header_test.csv'
    # df = pd.read_csv(file, encoding='gbk')
    #
    # x = '男'
    # df['Sex'].fillna(x, inplace=True)
    # lst = []
    # for i in df['Sex'].tolist():
    #     lst.append([i])
    # enc = preprocessing.OrdinalEncoder().fit(lst)
    #
    # y = enc.transform(lst)
    # print(y)

if __name__ == "__main__":
    ordinal()



